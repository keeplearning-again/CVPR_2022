{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import arxiv\n",
    "import yaml\n",
    "import logging\n",
    "import argparse\n",
    "import datetime\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='[%(asctime)s %(levelname)s] %(message)s',\n",
    "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                    level=logging.INFO)\n",
    "\n",
    "base_url = \"https://arxiv.paperswithcode.com/api/v0/papers/\"\n",
    "github_url = \"https://api.github.com/search/repositories\"\n",
    "arxiv_url = \"http://arxiv.org/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_authors(authors, first_author = False):\n",
    "    output = str()\n",
    "    if first_author == False:\n",
    "        output = \", \".join(str(author) for author in authors)\n",
    "    else:\n",
    "        output = authors[0]\n",
    "    return output\n",
    "def sort_papers(papers):\n",
    "    output = dict()\n",
    "    keys = list(papers.keys())\n",
    "    keys.sort(reverse=True)\n",
    "    for key in keys:\n",
    "        output[key] = papers[key]\n",
    "    return output    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_code_link(qword:str) -> str:\n",
    "    \"\"\"\n",
    "    This short function was auto-generated by ChatGPT. \n",
    "    I only renamed some params and added some comments.\n",
    "    @param qword: query string, eg. arxiv ids and paper titles\n",
    "    @return paper_code in github: string, if not found, return None\n",
    "    \"\"\"\n",
    "    # query = f\"arxiv:{arxiv_id}\"\n",
    "    query = f\"{qword}\"\n",
    "    params = {\n",
    "        \"q\": query,\n",
    "        \"sort\": \"stars\",\n",
    "        \"order\": \"desc\"\n",
    "    }\n",
    "    r = requests.get(github_url, params=params)\n",
    "    results = r.json()\n",
    "    code_link = None\n",
    "    if results[\"total_count\"] > 0:\n",
    "        code_link = results[\"items\"][0][\"html_url\"]\n",
    "    return code_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "file_path = \"CVPR_2022_accepted_papers.xls\"\n",
    "df = pd.read_excel(file_path, 'Sheet1', header=0) \n",
    "data_array = np.array(df)\n",
    "article_name_list = data_array[:, 0].reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = article_name_list[1]\n",
    "max_results = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[03/10/2023 19:59:30 INFO] Requesting 1 results at offset 0\n",
      "[03/10/2023 19:59:30 INFO] Requesting page of results\n",
      "[03/10/2023 19:59:31 INFO] Got first page; 1 of 1 results available\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time = 2022-03-28 title = Compositional Temporal Grounding with Structured Variational Cross-Graph Correspondence Learning author = Juncheng Li\n",
      "http://arxiv.org/abs/2203.13049v2\n"
     ]
    }
   ],
   "source": [
    "# test for the search_engine.results()\n",
    "search_engine = arxiv.Search(\n",
    "        query = query,\n",
    "        max_results = max_results,\n",
    "        sort_by = arxiv.SortCriterion.Relevance\n",
    "    )\n",
    "for result in search_engine.results():\n",
    "\n",
    "        paper_id            = result.get_short_id()\n",
    "        paper_title         = result.title\n",
    "        paper_url           = result.entry_id\n",
    "        code_url            = base_url + paper_id #TODO\n",
    "        paper_abstract      = result.summary.replace(\"\\n\",\" \")\n",
    "        paper_authors       = get_authors(result.authors)\n",
    "        paper_first_author  = get_authors(result.authors,first_author = True)\n",
    "        primary_category    = result.primary_category\n",
    "        publish_time        = result.published.date()\n",
    "        update_time         = result.updated.date()\n",
    "        comments            = result.comment\n",
    "        print(f\"Time = {update_time} title = {paper_title} author = {paper_first_author}\")\n",
    "        print(paper_url)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_papers_content(topic,query=\"slam\", max_results=2):\n",
    "    \"\"\"\n",
    "    @param topic: str\n",
    "    @param query: str\n",
    "    @return paper_with_code: dict\n",
    "    \"\"\"\n",
    "    # output \n",
    "    content = dict() \n",
    "    content_to_web = dict()\n",
    "    search_engine = arxiv.Search(\n",
    "        query = query,\n",
    "        max_results = max_results,\n",
    "        sort_by = arxiv.SortCriterion.Relevance\n",
    "    )\n",
    "    paper_pdf_url_list = []\n",
    "    for result in search_engine.results():\n",
    "\n",
    "        paper_id            = result.get_short_id()\n",
    "        paper_title         = result.title\n",
    "        paper_url           = result.entry_id\n",
    "        code_url            = base_url + paper_id #TODO\n",
    "        paper_abstract      = result.summary.replace(\"\\n\",\" \")\n",
    "        paper_authors       = get_authors(result.authors)\n",
    "        paper_first_author  = get_authors(result.authors,first_author = True)\n",
    "        primary_category    = result.primary_category\n",
    "        publish_time        = result.published.date()\n",
    "        update_time         = result.updated.date()\n",
    "        comments            = result.comment\n",
    "        print(f'successfully get {paper_title}')\n",
    "\n",
    "        # eg: 2108.09112v1 -> 2108.09112\n",
    "        ver_pos = paper_id.find('v')\n",
    "        if ver_pos == -1:\n",
    "            paper_key = paper_id\n",
    "        else:\n",
    "            paper_key = paper_id[0:ver_pos]    \n",
    "        paper_url = arxiv_url + 'abs/' + paper_key\n",
    "        # https://arxiv.org/pdf/2010.04159.pdf\n",
    "        paper_pdf_url = arxiv_url + 'pdf/' + paper_key + '.pdf'\n",
    "        https_paper_pdf_url = paper_pdf_url.split(':')[0] + 's:' + paper_pdf_url.split(':')[1]\n",
    "        paper_pdf_url_list.append(https_paper_pdf_url)\n",
    "        \n",
    "        try:\n",
    "            # source code link    \n",
    "            r = requests.get(code_url).json()\n",
    "            repo_url = None\n",
    "            if \"official\" in r and r[\"official\"]:\n",
    "                repo_url = r[\"official\"][\"url\"]\n",
    "            # TODO: not found, two more chances  \n",
    "            # else: \n",
    "            #    repo_url = get_code_link(paper_title)\n",
    "            #    if repo_url is None:\n",
    "            #        repo_url = get_code_link(paper_key)\n",
    "            if repo_url is not None:\n",
    "                content[paper_key] = \"|**{}**|**{}**|{} et.al.|[{}]({})|[PDF_link]({})|**[Code_link]({})**|\\n\".format(\n",
    "                       update_time,paper_title,paper_first_author,paper_key,paper_url,paper_pdf_url,repo_url)\n",
    "                content_to_web[paper_key] = \"- {}, **{}**, {} et.al., Paper: [{}]({}), Code: **[{}]({})**\".format(\n",
    "                       update_time,paper_title,paper_first_author,paper_url,paper_url,repo_url,repo_url)\n",
    "\n",
    "            else:\n",
    "                content[paper_key] = \"|**{}**|**{}**|{} et.al.|[{}]({})|null|\\n\".format(\n",
    "                       update_time,paper_title,paper_first_author,paper_key,paper_url)\n",
    "                content_to_web[paper_key] = \"- {}, **{}**, {} et.al., Paper: [{}]({})\".format(\n",
    "                       update_time,paper_title,paper_first_author,paper_url,paper_url)\n",
    "\n",
    "            # TODO: select useful comments\n",
    "            comments = None\n",
    "            if comments != None:\n",
    "                content_to_web[paper_key] += f\", {comments}\\n\"\n",
    "            else:\n",
    "                content_to_web[paper_key] += f\"\\n\"\n",
    "\n",
    "        except Exception as e:\n",
    "            print((f\"exception: {e} with id: {paper_key}\"))\n",
    "\n",
    "    data = {topic:content}\n",
    "    return data, paper_pdf_url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https//arxiv.org/pdf/2010.04159.pdf\n"
     ]
    }
   ],
   "source": [
    "paper_pdf_url = 'http://arxiv.org/pdf/2010.04159.pdf'\n",
    "print(paper_pdf_url.split(':')[0] + 's' + paper_pdf_url.split(':')[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_json_file(filename, data_all):\n",
    "    with open(filename,\"r\") as f:\n",
    "        content = f.read()\n",
    "        if not content:\n",
    "            m = {}\n",
    "        else:\n",
    "            m = json.loads(content)\n",
    "            \n",
    "    json_data = m.copy() \n",
    "    \n",
    "    # update papers in each keywords         \n",
    "    for data in data_all:\n",
    "        # print(type(data))\n",
    "        # print(data)\n",
    "        for keyword in data.keys():\n",
    "            papers = data[keyword]\n",
    "\n",
    "            if keyword in json_data.keys():\n",
    "                json_data[keyword].update(papers)\n",
    "            else:\n",
    "                json_data[keyword] = papers\n",
    "\n",
    "    with open(filename,\"w\") as f:\n",
    "        json.dump(json_data,f)\n",
    "    \n",
    "def json_to_md(filename):\n",
    "    \"\"\"\n",
    "    @param filename: str\n",
    "    @return None\n",
    "    \"\"\"\n",
    "    \n",
    "    DateNow = datetime.date.today()\n",
    "    DateNow = str(DateNow)\n",
    "    DateNow = DateNow.replace('-','.')\n",
    "    \n",
    "    with open(filename,\"r\") as f:\n",
    "        content = f.read()\n",
    "        if not content:\n",
    "            data = {}\n",
    "        else:\n",
    "            data = json.loads(content)\n",
    "\n",
    "    md_filename = \"README.md\"  \n",
    "      \n",
    "    # clean README.md if daily already exist else create it\n",
    "    with open(md_filename,\"w+\") as f:\n",
    "        pass\n",
    "\n",
    "    # write data into README.md\n",
    "    with open(md_filename,\"a+\") as f:\n",
    "  \n",
    "        f.write(\"## Updated on \" + DateNow + \"\\n\\n\")\n",
    "        \n",
    "        for keyword in data.keys():\n",
    "            day_content = data[keyword]\n",
    "            if not day_content:\n",
    "                continue\n",
    "            # the head of each part\n",
    "            f.write(f\"## {keyword}\\n\\n\")\n",
    "            f.write(\"|Publish Date|Title|Authors|Abstract|PDF|Code link|\\n\" + \"|---|---|---|---|---|---|\\n\")\n",
    "            # sort papers by date\n",
    "            day_content = sort_papers(day_content)\n",
    "        \n",
    "            for _,v in day_content.items():\n",
    "                if v is not None:\n",
    "                    f.write(v)\n",
    "\n",
    "            f.write(f\"\\n\")\n",
    "    print(\"finished\")     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "# from selenium import webdriver\n",
    "import requests\n",
    "import threading\n",
    "import os \n",
    "import time\n",
    "\n",
    "def Handler(start, end, url, filename): \n",
    "    # specify the starting and ending of the file \n",
    "    headers = {'Range': 'bytes=%d-%d' % (start, end)} \n",
    "    # request the specified part and get into variable     \n",
    "    r = requests.get(url, headers=headers, stream=True) \n",
    "    # open the file and write the content of the html page into file. \n",
    "    with open(filename, \"r+b\") as fp: \n",
    "        fp.seek(start) \n",
    "        var = fp.tell() \n",
    "        fp.write(r.content)\n",
    "\n",
    "def download_file(url_of_file,name,number_of_threads): \n",
    "    r = requests.head(url_of_file) \n",
    "    if name: \n",
    "        file_name = name \n",
    "    else: \n",
    "        file_name = url_of_file.split('/')[-1] \n",
    "    try: \n",
    "        file_size = int(r.headers['content-length']) \n",
    "    except: \n",
    "        print(\"Invalid URL\")\n",
    "        return\n",
    "\n",
    "    part = int(file_size) / number_of_threads \n",
    "    fp = open(file_name, \"wb\") \n",
    "    fp.close() \n",
    "    for i in range(number_of_threads): \n",
    "        start = int(part * i) \n",
    "        end = int(start + part) \n",
    "        # create a Thread with start and end locations \n",
    "        t = threading.Thread(target=Handler, \n",
    "            kwargs={'start': start, 'end': end, 'url': url_of_file, 'filename': file_name}) \n",
    "        t.setDaemon(True) \n",
    "        t.start() \n",
    "\n",
    "    main_thread = threading.current_thread() \n",
    "    for t in threading.enumerate(): \n",
    "        if t is main_thread: \n",
    "            continue\n",
    "        t.join() \n",
    "    print('finish one')\n",
    "\n",
    "save_path = 'C:/Users/14541/Desktop/CVPR_2022_articles'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[03/10/2023 20:02:42 INFO] Requesting 1 results at offset 0\n",
      "[03/10/2023 20:02:42 INFO] Requesting page of results\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword: Cascade Transformers for End-to-End Person Search\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[03/10/2023 20:02:43 INFO] Got first page; 1 of 1 results available\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully get Cascade Transformers for End-to-End Person Search\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[03/10/2023 20:02:44 INFO] Requesting 1 results at offset 0\n",
      "[03/10/2023 20:02:44 INFO] Requesting page of results\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Keyword: Compositional Temporal Grounding with Structured Variational Cross-Graph Correspondence Learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[03/10/2023 20:02:45 INFO] Got first page; 1 of 1 results available\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully get Compositional Temporal Grounding with Structured Variational Cross-Graph Correspondence Learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[03/10/2023 20:02:46 INFO] Requesting 1 results at offset 0\n",
      "[03/10/2023 20:02:46 INFO] Requesting page of results\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Keyword: Long-Tailed Recognition via Weight Balancing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[03/10/2023 20:02:47 INFO] Got first page; 1 of 1 results available\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully get Long-Tailed Recognition via Weight Balancing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[03/10/2023 20:02:47 INFO] Requesting 1 results at offset 0\n",
      "[03/10/2023 20:02:47 INFO] Requesting page of results\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Keyword: InfoGCN: Representation Learning for Human Skeleton-based Action Recognition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[03/10/2023 20:02:48 INFO] Got first page; 1 of 1 results available\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully get Improving Human Action Recognition by Non-action Classification\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[03/10/2023 20:02:49 INFO] Requesting 1 results at offset 0\n",
      "[03/10/2023 20:02:49 INFO] Requesting page of results\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Keyword: Interactive Geometry Editing of Neural Radiance Fields\n"
     ]
    }
   ],
   "source": [
    "data_collector = []\n",
    "keywords = dict()\n",
    "for i in article_name_list:\n",
    "    keywords[f\"{i}\"] = i\n",
    "pdf_url_list = []\n",
    "for topic, keyword in keywords.items():\n",
    " \n",
    "    print(\"Keyword: \" + topic)\n",
    "    data, pdf_url_list = get_papers_content(topic, query = keyword, max_results = 1)\n",
    "    data_collector.append(data)\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "# update README.md file\n",
    "json_file = \"cv-arxiv-daily.json\"\n",
    "if ~os.path.exists(json_file):\n",
    "    with open(json_file,'w')as a:\n",
    "        print(\"create \" + json_file)\n",
    "# update json data\n",
    "update_json_file(json_file, data_collector)\n",
    "# json data to markdown\n",
    "json_to_md(json_file)\n",
    "# for pdf_url in pdf_url_list:\n",
    "#     # download pdf using pdf_url\n",
    "#     filename = pdf_url[-14:]\n",
    "#     print('filename:{}, pdf_url:{}.'.format(filename,pdf_url))\n",
    "\n",
    "#     # pdf_url = 'https://arxiv.org/pdf/1709.06508.pdf'\n",
    "\n",
    "#     print('\\nDownloading {} ...'.format(filename))\n",
    "#     # pdf_url = 'https://arxiv.org/pdf/{}.pdf'.format(arxiv_id)\n",
    "#     # filename = filename_replace(paper_title) + '.pdf'\n",
    "#     ts = time.time()\n",
    "#     download_file(url_of_file=pdf_url, name=os.path.join(save_path,filename),number_of_threads=1) \n",
    "#     te = time.time()\n",
    "#     print('{:.0f}s [Complete] {}'.format(te-ts, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
